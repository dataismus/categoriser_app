{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "#client_bq = bigquery.Client.from_service_account_json(\"./credentials.json\", project='charged-dialect-824')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_load = load_bq_data(sql)\\nlen(df_load)\\ndf_load.head()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_bq_data(_sql):\n",
    "    _df = client_bq.query(_sql).to_dataframe()\n",
    "    return _df\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM RicardoInterview.product_detection_training_data\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"df_load = load_bq_data(sql)\n",
    "len(df_load)\n",
    "df_load.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief look at the sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw=pd.read_csv(\"ricardo_data.csv\", sep='|', index_col=0)\n",
    "raw=df_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d=defaultdict(list)\n",
    "for index, product in raw.productType.iteritems(): d[product].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>productType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-24471100344624315</td>\n",
       "      <td>Metal Hurlant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4941423510709663049</td>\n",
       "      <td>Mikrophone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sound_card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6882901079846443092</td>\n",
       "      <td>Sound Blaster Audigy CREATIVE</td>\n",
       "      <td>Soundkarte für PC</td>\n",
       "      <td>sound_card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-309151326745158426</td>\n",
       "      <td>Umlenkrolle Forst, extra leicht</td>\n",
       "      <td>100 kn / 10 Tonnen</td>\n",
       "      <td>winch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3342786788349504714</td>\n",
       "      <td>MATSCHFACH MORITZ SPEZIAL</td>\n",
       "      <td>*GRATIS LIEFERUNG*</td>\n",
       "      <td>sandpit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             articleId                            title            subtitle  \\\n",
       "0   -24471100344624315                    Metal Hurlant                 NaN   \n",
       "1 -4941423510709663049                       Mikrophone                 NaN   \n",
       "2  6882901079846443092    Sound Blaster Audigy CREATIVE   Soundkarte für PC   \n",
       "3  -309151326745158426  Umlenkrolle Forst, extra leicht  100 kn / 10 Tonnen   \n",
       "4 -3342786788349504714        MATSCHFACH MORITZ SPEZIAL  *GRATIS LIEFERUNG*   \n",
       "\n",
       "  productType  \n",
       "0    magazine  \n",
       "1  sound_card  \n",
       "2  sound_card  \n",
       "3       winch  \n",
       "4     sandpit  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37567 labeled samples for 383 (!) target classes\n"
     ]
    }
   ],
   "source": [
    "y=np.array(raw.productType)\n",
    "classes=np.unique(y)\n",
    "print(\"{} labeled samples for {} (!) target classes\".format(len(y), len(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of the frequencies of each product type in the sample set allows us to quickly confirm:\n",
    "1. whether there are imbalannces in the representation of target classes: Rather not, as all classes are represented by 50-99 samples, i.e. not even a single order of magnitude apart - this is quite a balanced set for a multiclass classifier.\n",
    "2. whether there are serious outliers or even misspells etc in the taxonomy: no, the Product type column is clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(raw.productType.value_counts().hist()) # all classes represented n times, where 48<n<99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(marble        48\n",
       " dump_truck    52\n",
       " jukebox       60\n",
       " Name: productType, dtype: int64,\n",
       " smartwatch     99\n",
       " plant          99\n",
       " coffee_mill    99\n",
       " Name: productType, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Indeed:\n",
    "raw.productType.value_counts().sort_values()[:3], raw.productType.value_counts().sort_values()[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial transformation to a BoF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining **title & subtitle as a single feature string** (after addressing empty subtitles): the rough(!) assumption is that the subtitles are not strictly inferior in informativeness for the target class to the main titles, as Ricardo users are not prompted so when entering. This hypothesis is worth examining, but not now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.replace(np.nan,'', regex=True)\n",
    "raw[\"descr\"]=raw.title+\" \"+raw.subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire training corpus combined consists of 1669640 total characters in 255518 total words\n"
     ]
    }
   ],
   "source": [
    "all_of_it = raw.descr.str.cat(sep=' ')\n",
    "print(\"The entire training corpus combined consists of\", len(all_of_it), 'total characters in', len(all_of_it.split(\" \")), 'total words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Bag-of-words vectorization of the assembled title strings and subsequent further transformation of the vocabulary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy  # for lemmatization\n",
    "import nltk   # for word stemming\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import bof_utils\n",
    "lexicon = spacy.load(\"de_core_news_sm\")\n",
    "# lexicon = spacy.load(\"de\")\n",
    "bof_utils.nlp=lexicon\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk_stopwords_de = nltk.corpus.stopwords.words('german')\n",
    "spacy_stopwords_de = spacy.lang.de.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jawohl', ',', 'dies', 'Auto', 'hören', 'sich', ',', 'warum', 'nicht', ',', 'meinen', 'Mann', ',', 'haben', 'ich', 'auch', 'einen', 'gleich', '?']\n"
     ]
    }
   ],
   "source": [
    "print(bof_utils.lemmatizer(\"jawohl, dieses Auto gehört mir, warum nicht, mein Mann, haben Sie auch ein gleiches?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jawohl', 'dies', 'Auto', 'gehören', 'mir', 'warum', 'nicht', 'mein', 'Mann', 'haben', 'Sie', 'auch', 'ein', 'gleiches']\n"
     ]
    }
   ],
   "source": [
    "print(bof_utils.de_blob_lemmatizer(\"jawohl, dieses Auto gehört mir, warum nicht, mein Mann, haben Sie auch ein gleiches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43977, training set size: 37567 samples * 43977 features\n",
      "# of tokens automatically excluded from the vocabulary: 0\n"
     ]
    }
   ],
   "source": [
    "_, bag= bof_utils.BoF(data=raw.descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = bag.get_feature_names()\n",
    "corpus_vocab_count_dict={w:c for (w,c) in zip(vocab, bag.transform([all_of_it]).toarray()[0])}\n",
    "sorted_corpus_count=sorted(corpus_vocab_count_dict.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 969),\n",
       " ('gr', 1008),\n",
       " ('von', 1036),\n",
       " ('cm', 1201),\n",
       " ('und', 1418),\n",
       " ('neu', 1429),\n",
       " ('für', 2094),\n",
       " ('mit', 3646)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_corpus_count[-8:] # potentially uninformative, too frequent tokens (mostly German stopwords) at the top of the vocabulary pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0018m2', 1),\n",
       " ('002', 1),\n",
       " ('003160', 1),\n",
       " ('0041', 1),\n",
       " ('0057', 1),\n",
       " ('00687', 1),\n",
       " ('006r01264', 1),\n",
       " ('008r13021', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_corpus_count[:8] # potentially uninformative, too infrequent tokens at the bottom of the vocabulary pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants=defaultdict(lambda: defaultdict(dict))\n",
    "variants[\"default_vectorizer\"][\"vectorizers\"]=bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slightly more sophisticated corpus representation - dropping German stopwords:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**: Would be intriguing to build a language recognition pipeline based on this:\n",
    "https://data-science-blog.com/blog/2018/11/12/language-detecting-with-sklearn-by-determining-letter-frequencies/\n",
    "to treat titles written in French, English, German & Italian accordingly regarding stopwords/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43683, training set size: 37567 samples * 43683 features\n",
      "# of tokens automatically excluded from the vocabulary: 0\n",
      "# of stopwords that were effectively excluded : 543\n"
     ]
    }
   ],
   "source": [
    "stopwords=spacy_stopwords_de\n",
    "\n",
    "_,variants[\"stopwords\"][\"vectorizers\"]= bof_utils.BoF(data=raw.descr, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rescaling the data with tf-idf:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43683, training set size: 37567 samples * 43683 features\n",
      "# of tokens automatically excluded from the vocabulary: 0\n",
      "# of stopwords that were effectively excluded : 543\n"
     ]
    }
   ],
   "source": [
    "_, variants[\"w_tfidf\"][\"vectorizers\"]= bof_utils.BoF(data=raw.descr, tfidf= True, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**and force-excluding too frequent/infrequent terms from the vocabulary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16576, training set size: 37567 samples * 16576 features\n",
      "# of tokens automatically excluded from the vocabulary: 27107\n",
      "# of stopwords that were effectively excluded : 543\n"
     ]
    }
   ],
   "source": [
    "_, variants[\"w_tfidf_min2_max100\"][\"vectorizers\"]= bof_utils.BoF(data=raw.descr, tfidf= True,  min_df=2, max_df=100, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...and enriching the feature set with some n-grams:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41058, training set size: 37567 samples * 41058 features\n",
      "# of tokens automatically excluded from the vocabulary: 226585\n",
      "# of stopwords that were effectively excluded : 543\n"
     ]
    }
   ],
   "source": [
    "variant=\"w_tfidf_ngram_3\"\n",
    "\n",
    "_, variants[\"w_tfidf_ngram_3\"][\"vectorizers\"]= bof_utils.BoF(data=raw.descr, tfidf= True, min_df=2, ngram_range=(1,3), stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last, trying lemmatization on the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['einigen', 'gleich', 'heiß', 'lieben', 'vergehen', 'wahr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16613, training set size: 37567 samples * 16613 features\n",
      "# of tokens automatically excluded from the vocabulary: 31327\n",
      "# of stopwords that were effectively excluded : 332\n"
     ]
    }
   ],
   "source": [
    "for custom_tokenizer in [bof_utils.lemmatizer]:\n",
    "    variant=custom_tokenizer.__name__+\"_w_tfidf\"\n",
    "    _,variants[variant][\"vectorizers\"]= bof_utils.BoF(data=raw.descr,custom_tokenizer=custom_tokenizer, \n",
    "                          tfidf= True, min_df=2, stop_words=custom_tokenizer(\", \".join(stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some (basic) Classifiers on these BoF representations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    "**Observe the sparse nature of the bag-of-words data representation, which pretty much dictates the families of models applicable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<37567x16576 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 125785 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "X=variants[\"w_tfidf_min2_max100\"][\"vectorizers\"].transform(raw.descr)\n",
    "print(repr(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accuracy of random guessing:*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.26 % \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:.2f} % \".format(100/len(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    "... I 'd better beat this... :P\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With data from default_vectorizer\n",
      "(vocabulary size: 43977):\n",
      "                                          train vs. test\n",
      "f1_macro of default        MultinomialNB:  0.98     0.67\n",
      "                                          train vs. test\n",
      "f1_macro of default        SGDClassifier:  0.99     0.70\n",
      "\n",
      "With data from stopwords\n",
      "(vocabulary size: 43683):\n",
      "                                          train vs. test\n",
      "f1_macro of default        MultinomialNB:  0.98     0.68\n",
      "                                          train vs. test\n",
      "f1_macro of default        SGDClassifier:  0.99     0.71\n",
      "\n",
      "With data from w_tfidf\n",
      "(vocabulary size: 43683):\n",
      "                                          train vs. test\n",
      "f1_macro of default        MultinomialNB:  0.99     0.71\n",
      "                                          train vs. test\n",
      "f1_macro of default        SGDClassifier:  0.99     0.75\n",
      "\n",
      "With data from w_tfidf_min2_max100\n",
      "(vocabulary size: 16576):\n",
      "                                          train vs. test\n",
      "f1_macro of default        MultinomialNB:  0.94     0.71\n",
      "                                          train vs. test\n",
      "f1_macro of default        SGDClassifier:  0.95     0.74\n",
      "\n",
      "With data from w_tfidf_ngram_3\n",
      "(vocabulary size: 41058):\n",
      "                                          train vs. test\n",
      "f1_macro of default        MultinomialNB:  0.95     0.71\n",
      "                                          train vs. test\n",
      "f1_macro of default        SGDClassifier:  0.97     0.75\n",
      "\n",
      "With data from lemmatizer_w_tfidf\n",
      "(vocabulary size: 16613):\n",
      "                                          train vs. test\n",
      "f1_macro of default        MultinomialNB:  0.91     0.64\n",
      "                                          train vs. test\n",
      "f1_macro of default        SGDClassifier:  0.95     0.67\n"
     ]
    }
   ],
   "source": [
    "import nested_x_val\n",
    "score_metric=\"f1_macro\"\n",
    "\n",
    "models=[MultinomialNB(alpha=.1) , SGDClassifier(random_state=101, loss=\"modified_huber\")] # LogisticRegression(C=1., warm_start=True, class_weight='balanced'),\n",
    "for variant in variants.keys():\n",
    "    print(\"\\nWith data from {}\\n(vocabulary size: {}):\".format(variant,len(variants[variant][\"vectorizers\"].get_feature_names())))\n",
    "    _=nested_x_val.model_mvp(models,\n",
    "                            variants[variant][\"vectorizers\"].transform(raw.descr),y, \n",
    "                            n_splits=3, n_jobs=1, score_metric=score_metric, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some further conclusions from the prototyping:**\n",
    "1. ... waaaay overfitting, at this point - mode training data will smoothen the BoF-tf-idf vector and increase accuracy\n",
    "2. the **w_tfidf_min2_max100** settings for the BoF vectorization seem the best early choice: competitive accuracy but a severely downsized vocabulary size (-60% of the rest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning of estimators on chosen representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i.  Choosing between the SVC and the NB**\n",
    " \n",
    "**ii. Simultaneously applying a basic (cross-validated) grid search for optimal settings on their main regularization hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_alpha_range=[0.01,0.1,1.,10.]\n",
    "SVM_alpha_range=[0.00001,0.0001,0.001,0.01]\n",
    "max_df_range=[50,100]\n",
    "min_df_range=[2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the sake of validation of our choice of Bag-of-words specifics, we will actually run the hyper-tuning grid search on the entire pipeline of {data transformation --> classification}:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1_macro on the test set: 0.718\n",
      "Average f1_macro score on the train set: 0.951\n"
     ]
    }
   ],
   "source": [
    "e2e_pipe= Pipeline([('vectorizer',CountVectorizer()),('classifier', MultinomialNB())])\n",
    "pipe_grid= [{\"classifier\":[MultinomialNB()], \"vectorizer\": [CountVectorizer(), TfidfVectorizer()],\n",
    "            \"classifier__alpha\": NB_alpha_range, \"vectorizer__max_df\":max_df_range,\"vectorizer__min_df\":min_df_range, \"vectorizer__stop_words\":[None, stopwords]},\n",
    "            {\"classifier\":[SGDClassifier(random_state=101, loss=\"modified_huber\")], \"vectorizer\": [CountVectorizer(), TfidfVectorizer()],\n",
    "            \"classifier__alpha\": SVM_alpha_range,\"vectorizer__max_df\":max_df_range,\"vectorizer__min_df\":min_df_range, \"vectorizer__stop_words\":[None, stopwords]}]\n",
    "\n",
    "best_e2e = nested_x_val.nested_x_val_grid_search(e2e_pipe, raw.descr, y, pipe_grid, show_me_params= False, score_metric=score_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For a more comprehensive review of the final predictor (end-to-end pipeline), we can review a classification report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model average  precision :    0.76\n",
      "Trained model average     recall :    0.72\n",
      "Trained model average   f1-score :    0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw.descr,y,stratify=y, test_size=0.2,random_state=102)\n",
    "y_pred=best_e2e.fit(X_train,y_train).predict(X_test)\n",
    "report=classification_report(y_test, y_pred, output_dict=True, target_names=best_e2e.classes_)\n",
    "for metric in ['precision', 'recall', 'f1-score']: print(\"Trained model average {:>10} : {:>7.2f}\".format(metric,report['macro avg'][metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected for a very balanced training set, the model appears well calibrated, as precision & recall are aligned with the f1-score, and we can be confident of equivalent confidence when generalizing to new data.\n",
    "**The model is ready to be deployed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-fitting the best model on the entire available labelled set:\n",
    "final=best_e2e.fit(raw.descr,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prototype vectorizer:\n",
      "\n",
      " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=100, max_features=None,\n",
      "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Our prototype vectorizer:\\n\\n\",final[\"vectorizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prototype classifier:\n",
      "\n",
      " SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=101, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Our prototype classifier:\\n\\n\",final[\"classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready for top-5 Product types predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_example_title=\"Blaster Moritz Mikrophone Batman Batman Woot 10 100 Dimitrios Batman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BoF representation of the to-be-classified example:\n",
      "{'moritz': 0.27393352620603156, 'mikrophone': 0.29401675926121484, 'blaster': 0.24597129091757353, 'batman': 0.8820502777836445}\n"
     ]
    }
   ],
   "source": [
    "X_example=final[\"vectorizer\"].transform([an_example_title])\n",
    "vocab_in_example=list(X_example.nonzero()[1])   # remember, the sample is of sparse matrix type, NOT a dense array or list!\n",
    "vocab=final[\"vectorizer\"].get_feature_names()\n",
    "print(\"The BoF representation of the to-be-classified example:\")\n",
    "print({vocab[i]:X_example[0, i] for i in vocab_in_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: \"Blaster Moritz Mikrophone Batman Batman Woot 10 100 Dimitrios Batman\" \n",
      "\n",
      "top_5_results:\n",
      "[\"product_type: 'bathrobe' with probability: 0.38 \", \"product_type: 'sound_card' with probability: 0.26 \", \"product_type: 'comic_book' with probability: 0.075 \", \"product_type: 'video_game_console' with probability: 0.056 \", \"product_type: 'sandpit' with probability: 0.054 \"]\n"
     ]
    }
   ],
   "source": [
    "probas=final[\"classifier\"].predict_proba(X_example)[0]\n",
    "top5=np.argsort(probas)[-5:]\n",
    "results=[\"product_type: '{}' with probability: {:.2} \".format(x,y) for x,y in zip(reversed(classes[top5]),reversed(probas[top5]))]\n",
    "print('title: \"{}\" \\n\\ntop_5_results:\\n{}'.format(an_example_title, str(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this all needs to be packaged as a single output function for the flask service:\n",
    "import json\n",
    "def predict_top5(doc, classifier):\n",
    "    top5=np.argsort(classifier.predict_proba(doc)[0])[-5:]\n",
    "        \n",
    "    #requested JSON elements:\n",
    "    title=doc\n",
    "    top_5_results=[{\"product_type\": x, \"score\": \"{:.4f}\".format(y)} for (x,y) in zip(reversed(classes[top5]),reversed(probas[top5]))]\n",
    "    product_type=classes[top5][-1]\n",
    "    return json.dumps({\"title\": title, \"top_5_results\": top_5_results, \"product_type\": product_type}, indent=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"title\": [\n",
      "      \"Blaster Moritz Mikrophone Batman Batman Woot 10 100 Dimitrios Batman\"\n",
      "   ],\n",
      "   \"top_5_results\": [\n",
      "      {\n",
      "         \"product_type\": \"bathrobe\",\n",
      "         \"score\": \"0.3759\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"sound_card\",\n",
      "         \"score\": \"0.2574\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"comic_book\",\n",
      "         \"score\": \"0.0750\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"video_game_console\",\n",
      "         \"score\": \"0.0563\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"sandpit\",\n",
      "         \"score\": \"0.0544\"\n",
      "      }\n",
      "   ],\n",
      "   \"product_type\": \"bathrobe\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(predict_top5([\"Blaster Moritz Mikrophone Batman Batman Woot 10 100 Dimitrios Batman\"], final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trained.pkl', 'wb') as f: pickle.dump(final, f)  \n",
    "    # the BoF transformer is shipped over to the Flask service also as a component of the pipelined predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"title\": [\n",
      "      \"Blaster Moritz Mikrophone\"\n",
      "   ],\n",
      "   \"top_5_results\": [\n",
      "      {\n",
      "         \"product_type\": \"sound_card\",\n",
      "         \"score\": \"0.2574\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"sandpit\",\n",
      "         \"score\": \"0.0544\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"vhs\",\n",
      "         \"score\": \"0.0416\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"voucher\",\n",
      "         \"score\": \"0.0513\"\n",
      "      },\n",
      "      {\n",
      "         \"product_type\": \"bottle\",\n",
      "         \"score\": \"0.0192\"\n",
      "      }\n",
      "   ],\n",
      "   \"product_type\": \"sound_card\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def wrapper(classifier):\n",
    "    def top5(doc):\n",
    "        top5=np.argsort(classifier.predict_proba(doc)[0])[-5:]\n",
    "        \n",
    "        #requested JSON elements:\n",
    "        title=doc\n",
    "        top_5_results=[{\"product_type\": x, \"score\": \"{:.4f}\".format(y)} for (x,y) in zip(reversed(classes[top5]),reversed(probas[top5]))]\n",
    "        product_type=classes[top5][-1]\n",
    "        return json.dumps({\"title\": title, \"top_5_results\": top_5_results, \"product_type\": product_type}, indent=3)\n",
    "    return top5\n",
    "print(wrapper(final)([\"Blaster Moritz Mikrophone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
